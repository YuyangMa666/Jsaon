{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJm7FqXcTFaL"
   },
   "outputs": [],
   "source": [
    "[0.1 points]: All relevant ChatBot summaries [including link(s) to chat log histories if you're using ChatGPT] are reported within the notebook\n",
    "[0.3 points]: Well-communicated, clear demonstration of the \"model building\" process and techniques of \"Question 4\"\n",
    "[0.3 points]: Well-communicated, clear demonstration of the \"model building\" process and techniques of \"Question 7\"\n",
    "[0.3 points]: Well-communicated, clear demonstration of the \"model building\" process and techniques of \"Question 9\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is chatgpt link:https://chatgpt.com/share/67367577-0398-8008-9c3c-c0bdc4e8d970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGIfxYfhXLFv"
   },
   "source": [
    "### 1. Explain succinctly in your own words (but working with a ChatBot if needed)...\n",
    "the difference between Simple Linear Regression and Multiple Linear Regression; and the benefit the latter provides over the former\n",
    "the difference between using a continuous variable and an indicator variable in Simple Linear Regression; and these two linear forms\n",
    "the change that happens in the behavior of the model (i.e., the expected nature of the data it models) when a single indicator variable is introduced alongside a continuous variable to create a Multiple Linear Regression; and these two linear forms (i.e., the Simple Linear Regression versus the Multiple Linear Regression)\n",
    "the effect of adding an interaction between a continuous and an indicator variable in Multiple Linear Regression models; and this linear form\n",
    "the behavior of a Multiple Linear Regression model (i.e., the expected nature of the data it models) based only on indicator variables derived from a non-binary categorical variable; this linear form; and the necessarily resulting binary variable encodings it utilizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsfewvWtXMCG"
   },
   "source": [
    "\n",
    "1. **Simple vs. Multiple Linear Regression**:\n",
    "   - **Simple Linear Regression (SLR)** uses one independent variable to predict one dependent variable. It's like predicting your test score based on how many hours you studied.\n",
    "   - **Multiple Linear Regression (MLR)** uses more than one independent variable to make a prediction. It’s like predicting your test score based on how many hours you studied, how much you slept, and what you ate.\n",
    "\n",
    "   *Benefit of MLR over SLR*: MLR can consider many factors at once, giving a more accurate prediction because it includes more of what affects the outcome.\n",
    "\n",
    "2. **Continuous vs. Indicator Variables in Regression**:\n",
    "   - A **continuous variable** is something like age or income—values can range smoothly across a spectrum.\n",
    "   - An **indicator variable** is a simple yes/no variable, like if a car is red or not. It splits the data into groups (e.g., red cars vs. non-red cars).\n",
    "\n",
    "   *Difference in Use*: Continuous variables change the slope of the line in SLR, showing how one variable increases or decreases with another. Indicator variables shift the line up or down to show differences between groups.\n",
    "\n",
    "3. **Introducing an Indicator Variable in MLR**:\n",
    "   - When you add an indicator variable to a model that already has a continuous variable, you can show different starting points or trends for different groups. For example, it could show if urban and rural areas have different starting incomes but the same rate of income growth with age.\n",
    "\n",
    "4. **Adding an Interaction in MLR**:\n",
    "   - An interaction between a continuous variable and an indicator variable lets the model have different slopes for different groups. This could show, for example, that as people age, the effect on income is different for men and women.\n",
    "\n",
    "5. **MLR with Only Indicator Variables**:\n",
    "   - When MLR uses only indicator variables from a non-binary category (like type of pet: cat, dog, bird), it splits the data into separate groups and compares them. It uses binary encoding to handle categories by making each category a new yes/no variable, showing how each group is different from a reference group (the category not shown).\n",
    "\n",
    "These explanations aim to present a clearer, more straightforward understanding of how these regression models work and how different types of variables affect them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFp053t3XW-5"
   },
   "source": [
    "### 2. Explain in your own words (but working with a ChatBot if needed) what the specific (outcome and predictor) variables are for the scenario below; whether or not any meaningful interactions might need to be taken into account when predicting the outcome; and provide the linear forms with and without the potential interactions that might need to be considered\n",
    "Imagine a company that sells sports equipment. The company runs advertising campaigns on TV and online platforms. The effectiveness of the TV ad might depend on the amount spent on online advertising and vice versa, leading to an interaction effect between the two advertising mediums.\n",
    "\n",
    "Explain how to use these two formulas to make predictions of the outcome, and give a high level explaination in general terms of the difference between predictions from the models with and without the interaction\n",
    "\n",
    "Explain how to update and use the implied two formulas to make predictions of the outcome if, rather than considering two continuous predictor variables, we instead suppose the advertisement budgets are simply categorized as either \"high\" or \"low\" (binary variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJT2sQ5chsXu"
   },
   "source": [
    "#### Formula without Interaction\n",
    "\\[\n",
    "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( Y \\): Outcome variable\n",
    "- \\( \\beta_0 \\): Intercept term\n",
    "- \\( \\beta_1 \\): Coefficient for \\( X_1 \\)\n",
    "- \\( \\beta_2 \\): Coefficient for \\( X_2 \\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formula with Interaction\n",
    "\\[\n",
    "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1 \\times X_2)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( Y \\): Outcome variable\n",
    "- \\( \\beta_0 \\): Intercept term\n",
    "- \\( \\beta_1 \\): Coefficient for \\( X_1 \\)\n",
    "- \\( \\beta_2 \\): Coefficient for \\( X_2 \\)\n",
    "- \\( \\beta_3 \\): Coefficient for the interaction term \\( X_1 \\times X_2 \\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High-Level Differences in Prediction Outcomes\n",
    "Continuous vs. Binary Predictors: Continuous variables allow for fine-grained predictions based on exact budget amounts. Binary predictors simplify the model to estimate only whether \"high\" or \"low\" spending impacts the outcome, reducing the detail.\n",
    "With vs. Without Interaction (for both continuous and binary): Including interaction allows the model to consider the possibility that the effect of one budget level (high/low) depends on the level of the other, reflecting combined effects that may not be apparent in a model without interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdNAnDWYhs1B"
   },
   "source": [
    "### 3. Use smf to fit multiple linear regression models to the course project dataset from the canadian social connection survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define additive model formula\n",
    "additive_formula = \"social_connection_score ~ age + is_employed + C(education_level)\"\n",
    "\n",
    "# Fit additive model\n",
    "additive_model = smf.ols(formula=additive_formula, data=df).fit()\n",
    "\n",
    "# Display summary of the additive model\n",
    "print(additive_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define interaction model formula\n",
    "interaction_formula = \"social_connection_score ~ age * is_employed + C(education_level)\"\n",
    "\n",
    "# Fit interaction model\n",
    "interaction_model = smf.ols(formula=interaction_formula, data=df).fit()\n",
    "\n",
    "# Display summary of the interaction model\n",
    "print(interaction_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Interpreting the Models\n",
    "Additive Model Interpretation\n",
    "\n",
    "In the additive model, each predictor is interpreted independently:\n",
    "\n",
    "Age: The coefficient for age represents the expected change in social_connection_score for a one-unit increase in age, holding other variables constant.\n",
    "Employment Status (is_employed): The coefficient for is_employed shows the difference in social connection score between employed and non-employed individuals.\n",
    "Education Level (education_level): Each category’s coefficient (compared to the baseline) indicates how education level affects the social connection score.\n",
    "Interaction Model Interpretation\n",
    "\n",
    "The interaction term (age * is_employed) in the synergistic model indicates whether the effect of age on social_connection_score differs depending on is_employed status:\n",
    "\n",
    "If the interaction term is significant, it suggests that the relationship between age and social_connection_score is modified by employment status.\n",
    "Step 4: Statistical Evidence (p-values) for Each Predictor\n",
    "Review the output for each model:\n",
    "\n",
    "Significant Predictors: A low p-value (<0.05) indicates that a predictor is statistically significant, suggesting a likely association with the outcome.\n",
    "Interaction Term: In the interaction model, if the p-value for the interaction term is significant, it confirms the necessity of including the interaction to accurately model the data.\n",
    "Step 5: Visualization with Plotly\n",
    "a) Additive Model Visualization\n",
    "\n",
    "For a simple additive model with age (continuous) and is_employed (binary), you can create a scatter plot of age vs. social_connection_score with a \"best fit\" line based on employment status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Interpretation of Visualizations\n",
    "Additive Model Plot: If the additive model’s lines for each employment status are close together or parallel, it suggests that employment status does not significantly modify the effect of age.\n",
    "Interaction Model Plot: If the lines diverge based on employment status in the interaction plot, this supports the importance of including the interaction term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e566pOG4htMf"
   },
   "source": [
    "### 4. Explain the apparent contradiction between the factual statements regarding the fit below that \"the model only explains 17.6% of the variability in the data\" while at the same time \"many of the coefficients are larger than 10 while having strong or very strong evidence against the null hypothesis of 'no effect'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "id": "1VPBRc_4huAp",
    "outputId": "0ee8039a-460a-4870-bd47-1fba02419841"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 07 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>3.50e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>02:50:00</td>     <th>  Log-Likelihood:    </th> <td> -3649.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   800</td>      <th>  AIC:               </th> <td>   7323.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   788</td>      <th>  BIC:               </th> <td>   7379.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                 <td></td>                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                       <td>   26.8971</td> <td>    5.246</td> <td>    5.127</td> <td> 0.000</td> <td>   16.599</td> <td>   37.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.2]</th>              <td>   20.0449</td> <td>    7.821</td> <td>    2.563</td> <td> 0.011</td> <td>    4.692</td> <td>   35.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.3]</th>              <td>   21.3662</td> <td>    6.998</td> <td>    3.053</td> <td> 0.002</td> <td>    7.629</td> <td>   35.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.4]</th>              <td>   31.9575</td> <td>    8.235</td> <td>    3.881</td> <td> 0.000</td> <td>   15.793</td> <td>   48.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.5]</th>              <td>    9.4926</td> <td>    7.883</td> <td>    1.204</td> <td> 0.229</td> <td>   -5.982</td> <td>   24.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.6]</th>              <td>   22.2693</td> <td>    8.709</td> <td>    2.557</td> <td> 0.011</td> <td>    5.173</td> <td>   39.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                    <td>    0.5634</td> <td>    0.071</td> <td>    7.906</td> <td> 0.000</td> <td>    0.423</td> <td>    0.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.2]</th> <td>   -0.2350</td> <td>    0.101</td> <td>   -2.316</td> <td> 0.021</td> <td>   -0.434</td> <td>   -0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.3]</th> <td>   -0.3067</td> <td>    0.093</td> <td>   -3.300</td> <td> 0.001</td> <td>   -0.489</td> <td>   -0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.4]</th> <td>   -0.3790</td> <td>    0.105</td> <td>   -3.600</td> <td> 0.000</td> <td>   -0.586</td> <td>   -0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.5]</th> <td>   -0.0484</td> <td>    0.108</td> <td>   -0.447</td> <td> 0.655</td> <td>   -0.261</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.6]</th> <td>   -0.3083</td> <td>    0.112</td> <td>   -2.756</td> <td> 0.006</td> <td>   -0.528</td> <td>   -0.089</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>337.229</td> <th>  Durbin-Watson:     </th> <td>   1.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2871.522</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.684</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>11.649</td>  <th>  Cond. No.          </th> <td>1.40e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.4e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                  &        HP        & \\textbf{  R-squared:         } &     0.176   \\\\\n",
       "\\textbf{Model:}                          &       OLS        & \\textbf{  Adj. R-squared:    } &     0.164   \\\\\n",
       "\\textbf{Method:}                         &  Least Squares   & \\textbf{  F-statistic:       } &     15.27   \\\\\n",
       "\\textbf{Date:}                           & Thu, 07 Nov 2024 & \\textbf{  Prob (F-statistic):} &  3.50e-27   \\\\\n",
       "\\textbf{Time:}                           &     02:50:00     & \\textbf{  Log-Likelihood:    } &   -3649.4   \\\\\n",
       "\\textbf{No. Observations:}               &         800      & \\textbf{  AIC:               } &     7323.   \\\\\n",
       "\\textbf{Df Residuals:}                   &         788      & \\textbf{  BIC:               } &     7379.   \\\\\n",
       "\\textbf{Df Model:}                       &          11      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                         & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                       &      26.8971  &        5.246     &     5.127  &         0.000        &       16.599    &       37.195     \\\\\n",
       "\\textbf{C(Generation)[T.2]}              &      20.0449  &        7.821     &     2.563  &         0.011        &        4.692    &       35.398     \\\\\n",
       "\\textbf{C(Generation)[T.3]}              &      21.3662  &        6.998     &     3.053  &         0.002        &        7.629    &       35.103     \\\\\n",
       "\\textbf{C(Generation)[T.4]}              &      31.9575  &        8.235     &     3.881  &         0.000        &       15.793    &       48.122     \\\\\n",
       "\\textbf{C(Generation)[T.5]}              &       9.4926  &        7.883     &     1.204  &         0.229        &       -5.982    &       24.968     \\\\\n",
       "\\textbf{C(Generation)[T.6]}              &      22.2693  &        8.709     &     2.557  &         0.011        &        5.173    &       39.366     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                    &       0.5634  &        0.071     &     7.906  &         0.000        &        0.423    &        0.703     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.2]} &      -0.2350  &        0.101     &    -2.316  &         0.021        &       -0.434    &       -0.036     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.3]} &      -0.3067  &        0.093     &    -3.300  &         0.001        &       -0.489    &       -0.124     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.4]} &      -0.3790  &        0.105     &    -3.600  &         0.000        &       -0.586    &       -0.172     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.5]} &      -0.0484  &        0.108     &    -0.447  &         0.655        &       -0.261    &        0.164     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.6]} &      -0.3083  &        0.112     &    -2.756  &         0.006        &       -0.528    &       -0.089     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 337.229 & \\textbf{  Durbin-Watson:     } &    1.505  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2871.522  \\\\\n",
       "\\textbf{Skew:}          &   1.684 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  11.649 & \\textbf{  Cond. No.          } & 1.40e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.4e+03. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.176\n",
       "Model:                            OLS   Adj. R-squared:                  0.164\n",
       "Method:                 Least Squares   F-statistic:                     15.27\n",
       "Date:                Thu, 07 Nov 2024   Prob (F-statistic):           3.50e-27\n",
       "Time:                        02:50:00   Log-Likelihood:                -3649.4\n",
       "No. Observations:                 800   AIC:                             7323.\n",
       "Df Residuals:                     788   BIC:                             7379.\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================================\n",
       "                                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------------\n",
       "Intercept                          26.8971      5.246      5.127      0.000      16.599      37.195\n",
       "C(Generation)[T.2]                 20.0449      7.821      2.563      0.011       4.692      35.398\n",
       "C(Generation)[T.3]                 21.3662      6.998      3.053      0.002       7.629      35.103\n",
       "C(Generation)[T.4]                 31.9575      8.235      3.881      0.000      15.793      48.122\n",
       "C(Generation)[T.5]                  9.4926      7.883      1.204      0.229      -5.982      24.968\n",
       "C(Generation)[T.6]                 22.2693      8.709      2.557      0.011       5.173      39.366\n",
       "Q(\"Sp. Def\")                        0.5634      0.071      7.906      0.000       0.423       0.703\n",
       "Q(\"Sp. Def\"):C(Generation)[T.2]    -0.2350      0.101     -2.316      0.021      -0.434      -0.036\n",
       "Q(\"Sp. Def\"):C(Generation)[T.3]    -0.3067      0.093     -3.300      0.001      -0.489      -0.124\n",
       "Q(\"Sp. Def\"):C(Generation)[T.4]    -0.3790      0.105     -3.600      0.000      -0.586      -0.172\n",
       "Q(\"Sp. Def\"):C(Generation)[T.5]    -0.0484      0.108     -0.447      0.655      -0.261       0.164\n",
       "Q(\"Sp. Def\"):C(Generation)[T.6]    -0.3083      0.112     -2.756      0.006      -0.528      -0.089\n",
       "==============================================================================\n",
       "Omnibus:                      337.229   Durbin-Watson:                   1.505\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2871.522\n",
       "Skew:                           1.684   Prob(JB):                         0.00\n",
       "Kurtosis:                      11.649   Cond. No.                     1.40e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.4e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "# fail https://github.com/KeithGalli/pandas/blob/master/pokemon_data.csv\n",
    "pokeaman = pd.read_csv(url)\n",
    "pokeaman\n",
    "\n",
    "\n",
    "model1_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation)', data=pokeaman) # MLR\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation) + Q(\"Sp. Def\"):C(Generation)', data=pokeaman)#MLR(Q(\"Sp. Def\"):C(Generation)' intersaction)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") * C(Generation)', data=pokeaman) # MLR only with intersaction\n",
    "\n",
    "model2_fit = model2_spec.fit()\n",
    "model2_fit.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRQpWc3cXcla"
   },
   "source": [
    "Low R^2 Value (17.6%): This indicates that the overall model, despite possibly having significant individual predictors, does not explain a large proportion of the variance in the dependent variable (HP). An R^2 value of 17.6% means that the model as a whole explains only about 17.6% of the variability in HP around its mean. This suggests that there are other factors or variables not included in the model that affect HP significantly, or that the relationship between the predictors and HP is inherently noisy or complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jPX4d06iiaP"
   },
   "source": [
    "### 5. Discuss the following (five cells of) code and results with a ChatBot and based on the understanding you arrive at in this conversation explain what the following (five cells of) are illustrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6HTwJmysij3x",
    "outputId": "5140d001-4ef2-4532-e6ba-240ef02c6b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.14771558304519905\n",
      "'Out of sample' R-squared: 0.21208501873920738\n",
      "'In sample' R-squared:     0.4670944211413348\n",
      "'Out of sample' R-squared: 0.0024853491144003963\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
    "\n",
    "# Replace \"NaN\" (in the \"Type 2\" column with \"None\")\n",
    "pokeaman.fillna('None', inplace=True)\n",
    "\n",
    "np.random.seed(130)\n",
    "pokeaman_train,pokeaman_test = \\\n",
    "  train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n",
    "pokeaman_train\n",
    "model_spec3 = smf.ols(formula='HP ~ Attack + Defense',\n",
    "                      data=pokeaman_train)\n",
    "model3_fit = model_spec3.fit()\n",
    "model3_fit.summary()\n",
    "yhat_model3 = model3_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model3_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model3)[0,1]**2)\n",
    "model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary'\n",
    "model4_linear_form += ' * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "# DO NOT try adding '* C(Generation) * C(Q(\"Type 1\")) * C(Q(\"Type 2\"))'\n",
    "# That's 6*18*19 = 6*18*19 possible interaction combinations...\n",
    "# ...a huge number that will blow up your computer\n",
    "\n",
    "model4_spec = smf.ols(formula=model4_linear_form, data=pokeaman_train)\n",
    "model4_fit = model4_spec.fit()\n",
    "model4_fit.summary()\n",
    "yhat_model4 = model4_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model4_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model4)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of Results\n",
    "Model Complexity and Predictive Power:\n",
    "Model 3 (additive model) is simpler and uses only Attack and Defense. Its lower R-squared (both in-sample and out-of-sample) suggests it may not capture the full complexity of factors affecting HP.\n",
    "Model 4 (interaction model) includes many interactions and shows a potentially higher R-squared, indicating it may explain more variance in HP. However, if the out-of-sample R-squared is substantially lower than the in-sample R-squared, it may indicate overfitting due to high complexity.\n",
    "Generalizability:\n",
    "Comparing in-sample and out-of-sample R-squared for each model indicates how well each model generalizes. A model with a small drop in out-of-sample R-squared generally generalizes better.\n",
    "Practicality:\n",
    "Model 4’s complexity could lead to overfitting, so interpreting the necessity of each interaction in Model 4’s output would be important to balance complexity with predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56LEKSXznZOZ"
   },
   "source": [
    "### 6. Work with a ChatBot to understand how the model4_linear_form (linear form specification of model4) creates new predictor variables as the columns of the so-called \"design matrix\" model4_spec.exog (model4_spec.exog.shape) used to predict the outcome variable model4_spec.endog and why the so-called multicollinearity in this \"design matrix\" (observed in np.corrcoef(model4_spec.exog)) contribues to the lack of \"out of sample\" generalization of predictions from model4_fit; then, explain this consisely in your own works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "csSkvn14naJX",
    "outputId": "124e237d-1816-4543-a83c-91652a61da66"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.20e+16</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.671  \\\\\n",
       "\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.20e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Cond. No.\" WAS 343.0 WITHOUT to centering and scaling\n",
    "model3_fit.summary()\n",
    "from patsy import center, scale\n",
    "\n",
    "model3_linear_form_center_scale = \\\n",
    "  'HP ~ scale(center(Attack)) + scale(center(Defense))'\n",
    "model_spec3_center_scale = smf.ols(formula=model3_linear_form_center_scale,\n",
    "                                   data=pokeaman_train)\n",
    "model3_center_scale_fit = model_spec3_center_scale.fit()\n",
    "model3_center_scale_fit.summary()\n",
    "# \"Cond. No.\" is NOW 1.66 due to centering and scaling\n",
    "model4_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Defense))'\n",
    "model4_linear_form_CS += ' * scale(center(Speed)) * Legendary'\n",
    "model4_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "# Legendary is an indicator, so we don't center and scale that\n",
    "\n",
    "model4_CS_spec = smf.ols(formula=model4_linear_form_CS, data=pokeaman_train)\n",
    "model4_CS_fit = model4_CS_spec.fit()\n",
    "model4_CS_fit.summary().tables[-1]  # Cond. No. is 2,250,000,000,000,000\n",
    "\n",
    "# The condition number is still bad even after centering and scaling\n",
    "# Just as the condition number was very bad to start with\n",
    "model4_fit.summary().tables[-1]  # Cond. No. is 12,000,000,000,000,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTEtkSZZr-xP"
   },
   "source": [
    "When fitting regression models, especially those with multiple predictors and interaction terms, it's crucial to understand how these variables contribute to the model's design matrix and potentially affect the model's performance and stability. Here, we delve into the issues of multicollinearity and conditioning as they pertain to Model 4's design matrix.\n",
    "\n",
    "### Understanding the Design Matrix (`model4_spec.exog`)\n",
    "\n",
    "In the context of linear regression, the design matrix, often denoted as `X` in mathematical formulations, contains the predictor variables of the dataset structured in a way that regression analysis can be efficiently performed. Each column represents a predictor variable, while each row corresponds to an observation.\n",
    "\n",
    "For Model 4, given by:\n",
    "\n",
    "```python\n",
    "model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "```\n",
    "\n",
    "This formula involves multiple interaction terms which significantly expand the dimensionality of the design matrix. Specifically, interaction terms create new predictor variables by multiplying the constituent terms together, thus exploring how the predictors' effects modulate each other.\n",
    "\n",
    "### Impact of Multicollinearity\n",
    "\n",
    "**Multicollinearity** occurs when one or more predictor variables in a multiple regression model are highly correlated. This correlation can make it difficult to ascertain the effect of each predictor on the outcome variable, as changes in one predictor are associated with changes in at least one other predictor. In terms of the design matrix, multicollinearity can lead to:\n",
    "- High variances for the estimated coefficients, which means that small changes in the data can lead to large changes in the model estimates.\n",
    "- Difficulty in inverting the matrix (X'X) during the computation of the regression coefficients, which is essential for ordinary least squares (OLS) regression.\n",
    "\n",
    "### Conditioning and Its Effects\n",
    "\n",
    "The **condition number** of the matrix provides a measure of how sensitive a function is to changes in input or errors, and it applies here to the sensitivity of the linear regression's output to errors in its input data. High condition numbers often indicate multicollinearity and can lead to numerical instability, which might explain the lack of out-of-sample generalization seen in `model4_fit`. High condition numbers suggest that the matrix inversion necessary for computing regression coefficients is unstable or ill-conditioned.\n",
    "\n",
    "In the provided code, even after centering and scaling the predictors (which often helps reduce multicollinearity by normalizing the scale of variables):\n",
    "```python\n",
    "model4_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Defense)) * scale(center(Speed)) * Legendary * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "```\n",
    "the condition number is still extremely high (`2,250,000,000,000,000`), indicating significant multicollinearity remains. This is likely due to the extensive interaction terms which include all combinations of multiple scaled and centered variables along with the `Legendary` indicator. This leads to a complex, high-dimensional space where the variables are not only inter-related but also scaled transformations of each other, contributing to the severe multicollinearity.\n",
    "\n",
    "### Consequences for Model Performance\n",
    "\n",
    "The extremely high condition number and observed multicollinearity contribute to the model's poor out-of-sample performance. Despite potentially high explanatory power within the sample (as indicated by in-sample R-squared), the model fails to generalize, suggesting it is overfitting to the noise and collinearity in the training data rather than capturing underlying patterns applicable more broadly. This is confirmed by the drastic drop in out-of-sample R-squared, indicating that the model's predictions are not reliable when faced with new data.\n",
    "\n",
    "### Summary\n",
    "\n",
    "This analysis highlights the complexity and challenges of building predictive models with high interaction and multicollinearity. It underscores the importance of careful variable selection, consideration of interaction terms, and diagnostics like condition numbers to ensure model stability and generalizability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Discuss with a ChatBot the rationale and principles by which `model5_linear_form` is  extended and developed from `model3_fit` and `model4_fit`; `model6_linear_form` is  extended and developed from `model5_linear_form`; and `model7_linear_form` is  extended and developed from `model6_linear_form`; then, explain this breifly and consisely in your own words<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "KwcJmONNrnJX",
    "outputId": "72b99227-1cdf-4e83-d31a-54bec23245c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3920134083531893\n",
      "'Out of sample' R-squared: 0.3001561448865219\n",
      "'In sample' R-squared:     0.3326310334310908\n",
      "'Out of sample' R-squared: 0.2957246042708008\n",
      "'In sample' R-squared:     0.37818209127432456\n",
      "'Out of sample' R-squared: 0.3505538923467793\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>252.300</td> <th>  Durbin-Watson:     </th> <td>   1.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3474.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.438</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.590</td>  <th>  Cond. No.          </th> <td>2.34e+09</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 252.300 & \\textbf{  Durbin-Watson:     } &    1.953  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3474.611  \\\\\n",
       "\\textbf{Skew:}          &   2.438 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  16.590 & \\textbf{  Cond. No.          } & 2.34e+09  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's something a little more reasonable...\n",
    "model5_linear_form = 'HP ~ Attack + Defense + Speed + Legendary'\n",
    "model5_linear_form += ' + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "model5_linear_form += ' + C(Generation) + C(Q(\"Type 1\")) + C(Q(\"Type 2\"))'\n",
    "\n",
    "model5_spec = smf.ols(formula=model5_linear_form, data=pokeaman_train)\n",
    "model5_fit = model5_spec.fit()\n",
    "model5_fit.summary()\n",
    "yhat_model5 = model5_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model5_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model5)[0,1]**2)\n",
    "# Here's something a little more reasonable...\n",
    "model6_linear_form = 'HP ~ Attack + Speed + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "# And here we'll add the significant indicators from the previous model\n",
    "# https://chatgpt.com/share/81ab88df-4f07-49f9-a44a-de0cfd89c67c\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model6_linear_form += ' + I(Generation==2)'\n",
    "model6_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model6_spec = smf.ols(formula=model6_linear_form, data=pokeaman_train)\n",
    "model6_fit = model6_spec.fit()\n",
    "model6_fit.summary()\n",
    "yhat_model6 = model6_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2)\n",
    "# And here's a slight change that seems to perhaps improve prediction...\n",
    "model7_linear_form = 'HP ~ Attack * Speed * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form += ' + I(Generation==2)'\n",
    "model7_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model7_spec = smf.ols(formula=model7_linear_form, data=pokeaman_train)\n",
    "model7_fit = model7_spec.fit()\n",
    "model7_fit.summary()\n",
    "yhat_model7 = model7_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2)\n",
    "# And here's a slight change that seems to perhas improve prediction...\n",
    "model7_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Speed))'\n",
    "model7_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "# We DO NOT center and scale indicator variables\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form_CS += ' + I(Generation==2)'\n",
    "model7_linear_form_CS += ' + I(Generation==5)'\n",
    "\n",
    "model7_CS_spec = smf.ols(formula=model7_linear_form_CS, data=pokeaman_train)\n",
    "model7_CS_fit = model7_CS_spec.fit()\n",
    "model7_CS_fit.summary().tables[-1]\n",
    "# \"Cond. No.\" is NOW 15.4 due to centering and scaling\n",
    "# \"Cond. No.\" WAS 2,340,000,000 WITHOUT to centering and scaling\n",
    "model7_fit.summary().tables[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW6UUWfnr8WL"
   },
   "source": [
    "The progression from **model3_fit** to **model7_fit** illustrates a strategic refinement of a statistical model in the quest for improved predictive power and more reliable generalization. Here's an overview of how each model extends from its predecessors, the rationale behind these extensions, and how they address specific concerns like multicollinearity and evidence of predictive associations.\n",
    "\n",
    "### Model Development Overview:\n",
    "\n",
    "1. **Model3_fit to Model5_fit**:\n",
    "   - **Extension**: **Model5_fit** extends **model3_fit** by incorporating additional predictors (Speed, Legendary, Special Defense, Special Attack) and categorical variables (Generation, Type 1, Type 2).\n",
    "   - **Rationale**: This extension aims to capture more of the variance in the outcome variable, HP, by introducing more dimensions that might influence HP. The inclusion of categorical variables accounts for potentially significant group effects, such as differences between Pokémon generations or types.\n",
    "   - **Multicollinearity**: By adding many predictors, especially categorical ones that can lead to a large increase in the number of dummy variables, there is a risk of introducing multicollinearity, but the model attempts to leverage the additional explanatory power despite this risk.\n",
    "\n",
    "2. **Model5_fit to Model6_fit**:\n",
    "   - **Extension**: **Model6_fit** simplifies the previous model by removing less significant predictors and focusing on significant categorical indicators (specific Types and Generations).\n",
    "   - **Rationale**: The streamlining is based on evidence from coefficient testing, which suggests that certain types and generations have more predictive power. This selective inclusion helps to manage multicollinearity while maintaining explanatory strength where it is most supported by data.\n",
    "   - **Generalizability**: This model likely improves generalizability by focusing on fewer, more impactful variables, reducing the chance of overfitting compared to **model5_fit**.\n",
    "\n",
    "3. **Model6_fit to Model7_fit**:\n",
    "   - **Extension**: **Model7_fit** introduces interaction terms among continuous variables (Attack, Speed, Sp. Def, Sp. Atk) while retaining significant categorical indicators.\n",
    "   - **Rationale**: The addition of interaction terms explores if the effect of one predictor on HP depends on another, aiming to capture complex relationships that might exist between these attributes.\n",
    "   - **Addressing Multicollinearity**: Although interactions can exacerbate multicollinearity, the strategic choice of which interactions to include is guided by their expected relevance to predicting HP, balancing complexity with predictive utility.\n",
    "\n",
    "### Addressing Multicollinearity:\n",
    "The condition numbers noted in the progression of these models provide insights into the severity of multicollinearity. While higher in more complex models, efforts to mitigate this through centering and scaling are evident in the use of transformed variables in **model7_fit**, substantially reducing the condition number and indicating better numerical stability. The \"condition number\" values are indicative of the multicollinearity levels, with efforts in model7 showing substantial improvement:\n",
    "\n",
    "- A condition number below 30 is generally not problematic, suggesting that while **model7_fit** has a condition number of 15.4, it's within an acceptable range, contrasting sharply with the exorbitant numbers seen without proper scaling.\n",
    "\n",
    "### Conclusion:\n",
    "Each step in the model development process aims to strike a balance between adding predictive power and avoiding overfitting. The progression from simpler to more complex models reflects a methodical approach to integrating more variables, refining the model based on statistical evidence, and addressing potential numerical issues through transformations. The models evolve from underfit scenarios, where they might not capture all relevant predictive associations, to more finely tuned models that better balance the complexities of the data with the needs for generalizable and stable prediction. The ultimate test, reflected in the out-of-sample R-squared values, shows incremental improvements, indicating that each successive model provides a better generalization from the training data to unseen data, culminating in model7's improved performance and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlZyMxaytDh1"
   },
   "source": [
    "### 8. Work with a ChatBot to write a for loop to create, collect, and visualize many different paired \"in sample\" and \"out of sample\" model performance metric actualizations (by not using np.random.seed(130) within each loop iteration); and explain in your own words the meaning of your results and purpose of this demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqoAMqlJswBz"
   },
   "source": [
    "To address the query about demonstrating the effects of random train-test splits on model performance, we'll construct a loop in Python that creates multiple realizations of \"in-sample\" and \"out-of-sample\" R-squared values for a linear regression model. This will help illustrate the variability in model performance due to different random splits of the data, without fixing a random seed. This variability can provide insights into the model's stability and generalizability.\n",
    "\n",
    "### Objective\n",
    "The purpose of this demonstration is to evaluate how sensitive the model performance is to different subsets of data chosen randomly. This can reveal whether the model is overfitting the training data and how well it might be expected to perform on unseen data.\n",
    "\n",
    "\n",
    "### Explanation of Results and Purpose\n",
    "- **Variability in R-squared**: This visualization shows each iteration's \"in-sample\" versus \"out-of-sample\" R-squared values. The closer these points lie to the diagonal line (y=x), the more consistent the model is between training and testing datasets.\n",
    "- **Interpretation**:\n",
    "  - **Points above the line**: Indicate instances where the model performs better on the test set than on the training set. While initially counterintuitive, this can occur due to particular splits that favorably align the test data with the model's structure, or when the training data happens to include more noise.\n",
    "  - **Points below the line**: Indicate overfitting, where the model is tuned too closely to the training data's idiosyncrasies and fails to generalize well.\n",
    "  \n",
    "- **General Observations**: Frequent occurrences of points significantly below the line across many random splits would suggest that the model consistently overfits. Conversely, consistent proximity to or above the line would suggest a robust model that generalizes well.\n",
    "\n",
    "This experiment highlights the importance of cross-validation and the need for careful model evaluation to ensure that findings are not merely artifacts of a particular random data split. By using multiple splits and observing the distribution of outcomes, we can more confidently assess the model's reliability and potential performance in practical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "xgpPNYxrs0fP",
    "outputId": "d8f8820a-e975-4469-ed68-35ddc200d427"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"1455c1d4-a6dd-4b5e-b620-4e6e301aa9b8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1455c1d4-a6dd-4b5e-b620-4e6e301aa9b8\")) {                    Plotly.newPlot(                        \"1455c1d4-a6dd-4b5e-b620-4e6e301aa9b8\",                        [{\"hovertemplate\":\"In Sample R-squared=%{x}\\u003cbr\\u003eOut of Sample R-squared=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[0.1678367784811714,0.1106338754112931,0.2180252435846014,0.2247685059961666,0.1930648587344811,0.08736776568684879,0.23288670034407188,0.11838142587409073,0.21932073090607662,0.23395968115467036,0.1837227507596677,0.31027605307729733,0.17446272568439947,0.19454495605237732,0.20091261610892508,0.26171058427318694,0.21353066204339555,0.15303386396357765,0.2252228876996677,0.15264261160233272,0.21455739745509583,0.3131236042002644,0.11163091513197654,0.1954061237698429,0.11945135996017253,0.2286780432439801,0.17323123371227722,0.1058249717287657,0.16885281804993102,0.16181430196730628,0.1953575872715917,0.17491554267906861,0.10546525630817372,0.2919347991723634,0.19898400701944596,0.19317604994505932,0.12150909379820063,0.17241361286553225,0.10024983253580644,0.20337755848793437,0.18068037141632498,0.2703559187986563,0.1705087365523824,0.17148768769584444,0.16655780525172037,0.22634742861243717,0.21136421183179366,0.20670943192350377,0.1871671832109303,0.26324712646007764,0.18320225374344212,0.23694459428466652,0.25630615717920613,0.2716887637087,0.18205482280932384,0.141759538846153,0.24198061060384424,0.17648764597980138,0.14966848288776502,0.2006304701237559,0.22886425932250987,0.15381251264049955,0.2538913451064774,0.11792735972745061,0.22429211012039985,0.2939251438246523,0.13060158478822037,0.22798833417562592,0.2447200668299837,0.19773583190707644,0.2673253617485081,0.2536299694216918,0.13752754334777695,0.15244343834795193,0.16138591281406034,0.33034811090554206,0.15213717679755445,0.15922989155807588,0.19670474961432105,0.10133192755710929,0.21810731762934843,0.1917559871631851,0.18744720091102907,0.25685201069064867,0.1805763884275714,0.2654183922468212,0.22547996051019703,0.15263476564557354,0.2782700630283059,0.1850585164778754,0.2838192390157759,0.18707008077761345,0.27368626393049433,0.2589550969743548,0.22696009842826081,0.14093145549821828,0.2076279120058292,0.17647168814257908,0.1536885621950781,0.19845606619460632],\"xaxis\":\"x\",\"y\":[0.19481970763399178,0.2883478435076267,0.1490203667260121,0.1430794278146116,0.1712811444035999,0.34018367409974326,0.14055728978296694,0.2913106904229935,0.15071971059355102,0.1399385477314447,0.17981783523676234,0.10617286974105981,0.18273674831304781,0.16777865085331528,0.16202566977690663,0.12450789434483327,0.15075088470100168,0.21254639647753645,0.14597816375527758,0.21156860407399872,0.15570405101088988,0.09151046146169467,0.295762848462822,0.16979887013345624,0.2546563607405623,0.1423587406180613,0.17867403947628702,0.289085702365421,0.19628123415138615,0.20038624028026003,0.16745214380032328,0.19050656937144048,0.2867474532764758,0.12158084790798165,0.15569845177543745,0.1695247133056129,0.26318720690745506,0.19225302864741842,0.32167588945876563,0.1616687510875496,0.18332481175245652,0.11891402573509359,0.19651974623394336,0.19016608698078666,0.19822264618370442,0.14292479667023117,0.1558828444492605,0.15480440831845868,0.16386533249787952,0.11226068097520847,0.1815380828501591,0.13331893164811995,0.12706400819819055,0.12140846017365449,0.18254047003955842,0.2318454604511801,0.13460381784706654,0.19053968212854375,0.22470209692537635,0.1577326423631331,0.14568919154868717,0.20304011854389623,0.12360405544137498,0.27859884563767306,0.13990675590000015,0.11251555799968249,0.2554881684115262,0.14867564858245805,0.13382468968090727,0.16506460907060225,0.11593590676729615,0.12793566224337305,0.23685207999416558,0.21218820306534392,0.19885526763604264,0.09536348289896675,0.2231686750492331,0.20611529913667495,0.16486321952751865,0.29286843046735384,0.15218662808080455,0.1735250509947412,0.17398616075380785,0.1284205744592806,0.1856242821515053,0.1256684962181535,0.14727137512592564,0.21548027653734345,0.1149858113224365,0.1774374725821576,0.11449679275374082,0.1753865414660696,0.10941778513246678,0.12674824954994782,0.14934070262385601,0.2276119962703254,0.16268136063116945,0.18935316310453706,0.21326138740887743,0.16875279206263555],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"line\":{\"dash\":\"dash\",\"shape\":\"linear\"},\"mode\":\"lines\",\"name\":\"y=x\",\"x\":[0,1],\"y\":[0,1],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"In Sample R-squared\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Out of Sample R-squared\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Model Performance Variation\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('1455c1d4-a6dd-4b5e-b620-4e6e301aa9b8');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Assuming 'pokeaman' DataFrame is preloaded\n",
    "reps = 100\n",
    "in_sample_Rsquared = np.zeros(reps)\n",
    "out_of_sample_Rsquared = np.zeros(reps)\n",
    "\n",
    "for i in range(reps):\n",
    "    pokeaman_train, pokeaman_test = train_test_split(pokeaman, train_size=0.5)\n",
    "    model_spec = smf.ols(formula='HP ~ Attack + Defense', data=pokeaman_train)\n",
    "    model_fit = model_spec.fit()\n",
    "\n",
    "    # In-sample R-squared\n",
    "    in_sample_Rsquared[i] = model_fit.rsquared\n",
    "\n",
    "    # Out-of-sample R-squared\n",
    "    predictions = model_fit.predict(pokeaman_test)\n",
    "    out_of_sample_Rsquared[i] = np.corrcoef(pokeaman_test['HP'], predictions)[0, 1] ** 2\n",
    "\n",
    "# Creating a DataFrame to hold results\n",
    "results_df = pd.DataFrame({\n",
    "    \"In Sample R-squared\": in_sample_Rsquared,\n",
    "    \"Out of Sample R-squared\": out_of_sample_Rsquared\n",
    "})\n",
    "\n",
    "# Plotting\n",
    "fig = px.scatter(results_df, x=\"In Sample R-squared\", y=\"Out of Sample R-squared\", title=\"Model Performance Variation\")\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], name=\"y=x\", mode='lines', line_shape='linear', line_dash='dash'))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGaBdVxUteSu"
   },
   "source": [
    "### 9. Work with a ChatBot to understand the meaning of the illustration below; and, explain this in your own word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d_XHvLq_tTK4",
    "outputId": "fde91172-f339-4375-b12d-6b44ec62ea10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.37818209127432456 (original)\n",
      "'Out of sample' R-squared: 0.00018905947562235405 (original)\n",
      "'In sample' R-squared:     0.5726118179916574 (gen1_predict_future)\n",
      "'Out of sample' R-squared: 0.11151363388299076 (gen1_predict_future)\n",
      "'In sample' R-squared:     0.37818209127432456 (original)\n",
      "'Out of sample' R-squared: 0.00018905947562235405 (original)\n",
      "'In sample' R-squared:     0.3904756578094535 (gen1to5_predict_future)\n",
      "'Out of sample' R-squared: 0.2339491543502001 (gen1to5_predict_future)\n",
      "'In sample' R-squared:     0.3326310334310908 (original)\n",
      "'Out of sample' R-squared: 0.00039016276723082656 (original)\n",
      "'In sample' R-squared:     0.44338805177272833 (gen1_predict_future)\n",
      "'Out of sample' R-squared: 0.19328585342762092 (gen1_predict_future)\n",
      "'In sample' R-squared:     0.3326310334310908 (original)\n",
      "'Out of sample' R-squared: 0.00039016276723082656 (original)\n",
      "'In sample' R-squared:     0.335172798241148 (gen1to5_predict_future)\n",
      "'Out of sample' R-squared: 0.2626269017880005 (gen1to5_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model7_gen1_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model7_gen1_predict_future_fit = model7_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model7_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")\n",
    "model7_gen1to5_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model7_gen1to5_predict_future_fit = model7_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model7_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")\n",
    "model6_gen1_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model6_gen1_predict_future_fit = model6_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model6_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")\n",
    "model6_gen1to5_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model6_gen1to5_predict_future_fit = model6_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model6_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1X-Hpn0vW2V"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7uyeEZFvV8f"
   },
   "source": [
    "The discussion you provided outlines an approach to understanding how different models perform when predicting future outcomes using historical data, segmented by a temporal variable such as \"Generation\" in the Pokémon dataset. This method of model evaluation emphasizes the importance of generalizability, especially when newer data might follow different trends than historical data. Let's break down the concept and implications illustrated by this approach.\n",
    "\n",
    "### Understanding the Sequential Data Approach\n",
    "\n",
    "The code provided uses a real-world, practical scenario where data is used to make predictions about future events. Here's a breakdown of the steps:\n",
    "\n",
    "1. **Model Training on Historical Data**: Models are trained on data from earlier generations (e.g., Generation 1 only, or Generations 1-5).\n",
    "2. **Prediction on Future Data**: These models are then used to predict outcomes for Pokémon from newer generations (e.g., Generation 2 onwards, or Generation 6 only), which were not included in the training data.\n",
    "\n",
    "This approach tests whether a model trained on past data can effectively predict future or unseen data. This is critical in many practical applications where models must perform well on data that may not exactly replicate the conditions or distributions of the training set.\n",
    "\n",
    "### Results Analysis\n",
    "\n",
    "The code compares the \"in-sample\" R-squared (model performance on training data) and \"out-of-sample\" R-squared (model performance on testing data) for both models (Model 6 and Model 7). The sequential approach reveals:\n",
    "\n",
    "- **Model Complexity vs. Generalizability**: While Model 7 may perform better on a mixed train-test set (as seen in previous tests), when it comes to predicting future generations, it may not perform as robustly as Model 6. This is likely due to Model 7's increased complexity, including high-order interactions that may capture noise or idiosyncratic patterns specific to the training data (\"overfitting\").\n",
    "- **Simpler Model Preference**: Model 6, being simpler, might not achieve as high \"in-sample\" R-squared values as Model 7 but shows more stable \"out-of-sample\" performance. This indicates better generalizability and reliability when applied to new generations.\n",
    "\n",
    "### Implications for Model Selection\n",
    "\n",
    "- **Interpretability and Parsimony**: Simpler models, like Model 6, are not only easier to interpret but may also offer sufficient predictive power with greater robustness to changes in data patterns. This is particularly important in fields where understanding the model's decision-making process is as important as its predictive accuracy.\n",
    "- **Future Predictions**: In scenarios where the model is used for forward-looking predictions, a balance between complexity and parsimony becomes crucial. Overly complex models may perform excellently on retrospective data but fail to adapt to new or evolving conditions.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The demonstration using sequential generation-based data splits serves to emphasize the importance of model parsimony and adaptability to new data. It showcases that while complex models can be tailored to maximize performance on known data, simpler models often provide more consistent and reliable predictions when faced with new or future data scenarios. This approach underlines a fundamental principle in predictive modeling: the best model is not always the one with the highest performance on historical data but rather the one that provides the most reliable and interpretable predictions across different sets of data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
